# AnÃ¡lisis de sentimiento y tÃ³picos en comentarios de estudiantes (Prepa en LÃ­nea, 2024-2025)  

## ğŸ” Resumen  
Proyecto de **Procesamiento de Lenguaje Natural (NLP)** que analiza comentarios de estudiantes hacia asesores virtuales de *Prepa en LÃ­nea SEP* (datos 2024-2025). Mediante **BERT** (fine-tuning para espaÃ±ol) y tÃ©cnicas de modelado de tÃ³picos, se identifican:  
- **Patrones de sentimiento** (clasificaciÃ³n binaria y multiclase).  
- **Temas crÃ­ticos** (ej: claridad de materiales, comunicaciÃ³n con asesores).  
- **Recomendaciones basadas en datos** para la mejora pedagÃ³gica.  

## ğŸ› ï¸ TecnologÃ­as Clave  
- **LibrerÃ­as NLP**: `transformers` (BERT), `spaCy` (procesamiento textual), `gensim` (LDA).  
- **AnalÃ­tica**: Pandas, NumPy, Scikit-learn.  
- **VisualizaciÃ³n**: Plotly (grÃ¡ficos interactivos), WordCloud.  
- **Infraestructura**: Jupyter Notebooks, Google Colab (GPU para fine-tuning de BERT).  

## ğŸ“Š MÃ©tricas y Hallazgos  
1. **DistribuciÃ³n de sentimientos**:  
   - 65% positivos, 25% neutros, 10% negativos (2025 vs 60%/28%/12% en 2024).  
2. **TÃ³picos recurrentes**:  
   - "RetroalimentaciÃ³n oportuna" (mÃ³dulo 3), "Dificultad en actividades" (mÃ³dulo 5).  
3. **BERT vs Baselines**:  
   - **Accuracy de BERT**: 92% vs 85% (Random Forest con TF-IDF).  

## ğŸ¯ Impacto  
- **Decisiones pedagÃ³gicas**: PriorizaciÃ³n de mÃ³dulos con mayor insatisfacciÃ³n.  
- **GestiÃ³n educativa**: DiseÃ±o de talleres para asesores basados en feedback negativo.  

## ğŸ“‚ Estructura del Repositorio  

data/
â”œâ”€â”€ raw/ # Comentarios crudos (CSV/JSON)
â”œâ”€â”€ processed/ # Datos limpios y vectorizados
notebooks/
â”œâ”€â”€ 1_EDA.ipynb # AnÃ¡lisis exploratorio
â”œâ”€â”€ 2_BERT_Sentiment.ipynb # Fine-tuning y evaluaciÃ³n
results/
â”œâ”€â”€ dashboards/ # GrÃ¡ficos interactivos
â”œâ”€â”€ models/ # Modelos guardados (BERT, LDA)

## ğŸ“Œ CÃ³mo Replicar el AnÃ¡lisis  
1. Instalar dependencias: `pip install -r requirements.txt`.  
2. Ejecutar notebooks en orden numÃ©rico.  
3. Para fine-tuning de BERT: Usar GPU (Google Colab recomendado).  

---

### ğŸ’¡ **Por quÃ© Destacar BERT?**  
- **PrecisiÃ³n superior**: Modelo de transformers ajustado para espaÃ±ol (ej: `dccuchile/bert-base-spanish-wwm`).  
- **Contexto lingÃ¼Ã­stico**: Captura ironÃ­a y negaciones (difÃ­cil con mÃ©todos clÃ¡sicos como TF-IDF).  
- **Transfer Learning**: Aplicable a otros proyectos educativos en espaÃ±ol.  

**Nota para reclutadores**: Si buscas detalles tÃ©cnicos adicionales (hiperparÃ¡metros de BERT, mÃ©tricas de evaluaciÃ³n), Â¡avÃ­same! Puedo ajustar el enfoque.  

--- 

